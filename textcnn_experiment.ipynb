{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后训练时间较长，调试好后转为py以命令行的方式运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Concatenate as concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dropout, Dense, Input\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import logging\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Flatten, Layer, InputSpec\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = False\n",
    "\n",
    "if test_mode:\n",
    "    fold_num = 2\n",
    "    max_epoch = 3\n",
    "else:\n",
    "    fold_num = 5\n",
    "    max_epoch = 50\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', \n",
    "                    level=logging.INFO, \n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "target_names = ['科技', '股票', '体育', '娱乐', '时政', '社会', '教育',\n",
    "               '财经', '家居', '游戏', '房产', '时尚', '彩票', '星座']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0.\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_true = self.y_val\n",
    "        y_pred = self.model.predict(self.x_val).argmax(axis=1)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        return f1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1 > self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs['val_f1'] = val_f1\n",
    "        print(f'val_f1: {val_f1:.5f}, best_val_f1: {self.best_val_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取训练集、测试集，并合并为total_df\n",
    "train_df = pd.read_csv('../data/train_set.csv', sep='\\t')\n",
    "test_df = pd.read_csv('../data/test_a.csv', sep='\\t')\n",
    "total_df = pd.concat([train_df['text'], test_df['text']], axis=0)\n",
    "\n",
    "# # 需要使用xxx.ipynb预先生成cut_train.csv和cut_test.csv\n",
    "# cut_train_df = pd.read_csv('../data/cut_train.csv', sep='\\t')\n",
    "# cut_test_df = pd.read_csv('../data/cut_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次需要运行这段 \n",
    "# max_len为句子的固定长度\n",
    "max_len = 3000\n",
    "tokenizer = Tokenizer()                   # 创建一个Tokenizer对象，将一个词转换为正整数\n",
    "tokenizer.fit_on_texts(total_df)  #将词编号，词频越大，编号越小\n",
    "vocab = tokenizer.word_index              # 得到每个词的编号\n",
    "\n",
    "x_test_word_ids = tokenizer.texts_to_sequences(test_set_df['text_set'])\n",
    "x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen=max_len, padding=\"post\", truncating=\"post\") \n",
    "\n",
    "raw_train_word_ids = tokenizer.texts_to_sequences(train_set_df['text_set'])\n",
    "#最终训练只需要raw_train_padded_seqs和raw_train_labels\n",
    "raw_train_padded_seqs = pad_sequences(raw_train_word_ids, maxlen=max_len, padding=\"post\", truncating=\"post\") \n",
    "raw_train_onehot_labels = keras.utils.to_categorical(train_df['label'], num_classes=14)\n",
    "\n",
    "# 存储中间数据后续使用节约时间\n",
    "pickle.dump(x_test_padded_seqs, open(\"../tmp_data/textcnn_maxlen3000_tpre1500_tpost1500_ppost_seq_test.pickle\", \"wb\"), protocol = 4)\n",
    "pickle.dump(raw_train_padded_seqs, open(\"../tmp_data/textcnn_maxlen3000_tpre1500_tpost1500_ppost_seq_train.pickle\", \"wb\"), protocol = 4)\n",
    "# 以下两项不需要每次都保存，如果每次保存也不影响结果\n",
    "pickle.dump(raw_train_onehot_labels, open(\"../tmp_data/textcnn_onehot_label_train.pickle\", \"wb\"))\n",
    "pickle.dump(tokenizer, open(\"../tmp_data/textcnn_tokenizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 第二次跑同样实验时候可以加载存储的中间数据\n",
    "# # x_test_padded_seqs = pickle.load(open(\"../tmp_data/textcnn_maxlen3000_tpre1500_tpost1500_ppost_seq_test.pickle\", \"rb\"))\n",
    "# raw_train_padded_seqs = pickle.load(open(\"../tmp_data/textcnn_maxlen3000_tpre1500_tpost1500_ppost_seq_train.pickle\", \"rb\"))\n",
    "# raw_train_onehot_labels = pickle.load(open(\"../tmp_data/textcnn_onehot_label_train.pickle\", \"rb\"))\n",
    "# tokenizer = pickle.load(open(\"../tmp_data/textcnn_tokenizer.pickle\", \"rb\"))\n",
    "# vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入预训练的word2vec，可选\n",
    "\n",
    "logging.info(\"Loading word2vec weight...\")\n",
    "# 需要使用xxx.ipynb训练得到word2vec词向量\n",
    "vectorPath = '../tmp_data/word2vec.d300.sg.w5.model' # 本地词向量的地址\n",
    "Word2VecModel = gensim.models.Word2Vec.load(vectorPath) # 读取词向量\n",
    "\n",
    "embeddings_matrix = np.zeros((len(vocab) + 1, Word2VecModel.vector_size))\n",
    "print(embeddings_matrix.shape)\n",
    "for word, index in vocab.items():\n",
    "    if word in Word2VecModel.wv.vocab.keys():\n",
    "        embeddings_matrix[index] = Word2VecModel.wv[word]  # 词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = fold_num\n",
    "monitor = 'val_f1'\n",
    "EMBEDDING_DIM = 300  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "skf = StratifiedKFold(n_splits=k_fold, random_state=2020, shuffle=True)\n",
    "test_pred = np.zeros((test_df.shape[0], 14), dtype=np.float32)\n",
    "\n",
    "logging.info(\"Start training...\")\n",
    "total_start =  time.time()\n",
    "\n",
    "for idx, (train_index, valid_index) in enumerate(skf.split(raw_train_padded_seqs, train_df['label'])):\n",
    "    x_train_padded_seqs, x_val_padded_seqs = raw_train_padded_seqs[train_index], raw_train_padded_seqs[valid_index]\n",
    "    one_hot_labels, val_one_hot_labels = raw_train_onehot_labels[train_index], raw_train_onehot_labels[valid_index]\n",
    "    y_train, y_val = train_df['label'].values[train_index], train_df['label'].values[valid_index]\n",
    "    \n",
    "    #构建textCNN\n",
    "    model_path = '../models/textcnn_maxlen3000_tpre1500_tpost1500_ppost_{}.h5'.format(idx+1)\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor=monitor, verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "    earlystopping = EarlyStopping(monitor=monitor, patience=5, verbose=1, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=2, mode='max', verbose=1)\n",
    "    main_input = Input(shape=(max_len,), dtype='float64')\n",
    "\n",
    "    # 嵌入层（使用预训练的词向量）\n",
    "    embedder = Embedding(input_dim = len(embeddings_matrix), output_dim = embeddings_matrix.shape[1], weights=[embeddings_matrix], input_length=max_len, trainable=False)\n",
    "    embed = embedder(main_input)\n",
    "    \n",
    "    # 卷积层和池化层，设置卷积核大小分别为3,4,5,6\n",
    "    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "    max1 = MaxPooling1D(pool_size=max_len)(cnn1)\n",
    "#     ave1 = AveragePooling1D(pool_size=max_len)(cnn1)\n",
    "    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "    max2 = MaxPooling1D(pool_size=max_len)(cnn2)\n",
    "#     ave2 = AveragePooling1D(pool_size=max_len)(cnn2)\n",
    "    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "    max3 = MaxPooling1D(pool_size=max_len)(cnn3)\n",
    "#     ave3 = AveragePooling1D(pool_size=max_len)(cnn3)\n",
    "    cnn4 = Conv1D(256, 10, padding='same', strides=1, activation='relu')(embed)\n",
    "    max4 = MaxPooling1D(pool_size=max_len)(cnn4)\n",
    "#     ave4 = AveragePooling1D(pool_size=max_len)(cnn4)\n",
    "#     cnn = concatenate(axis=-1)([max1, ave1, max2, ave2, max3, ave3, max4, ave4])\n",
    "    cnn = concatenate(axis=-1)([max1, max2, max3, max4])\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat) \n",
    "    main_output = Dense(14, activation='softmax')(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    start = time.time()\n",
    "    logging.info(\"Fold {} fitting...\".format(idx+1))\n",
    "    start = time.time()\n",
    "    model.fit(x_train_padded_seqs,\n",
    "              one_hot_labels, \n",
    "              batch_size=256,\n",
    "              validation_data=(x_val_padded_seqs, val_one_hot_labels), \n",
    "              callbacks=[Evaluator(validation_data=(x_val_padded_seqs, y_val)), checkpoint, reduce_lr, earlystopping],\n",
    "              verbose=2, \n",
    "              shuffle=True,\n",
    "              epochs=max_epoch)\n",
    "    end = time.time()\n",
    "    logging.info(\"fold {0} train {1:.3f} min\".format(idx+1, ((end - start) / 60)))\n",
    "    model.save('../models/textcnn_maxlen3000_tpre1500_tpost1500_ppost_model_{}.h5'.format(idx+1))\n",
    "    y_val_predict_result = model.predict(x_val_padded_seqs)  \n",
    "    y_val_predict_label = np.argmax(y_val_predict_result, axis=1) \n",
    "    logging.info(\"\\n{}\\n\".format(classification_report(y_val, y_val_predict_label, target_names=target_names)))\n",
    "    test_pred += model.predict(x_test_padded_seqs)\n",
    "total_end =  time.time()   \n",
    "logging.info(\"Total train time: {:.3f} min\".format((total_end - total_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "预测结果输出csv\n",
    "y_test_predict_label = np.argmax(test_pred, axis=1)\n",
    "test_df['label'] = y_test_predict_label\n",
    "test_df.to_csv('../results/textcnn_maxlen3000_tpre1500_tpost1500_ppost.csv', index=False, columns=['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
